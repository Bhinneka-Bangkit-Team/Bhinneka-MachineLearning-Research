{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coated-tiger",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinct-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-timber",
   "metadata": {},
   "source": [
    "# Read csv dataset that has been created, and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "separated-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\Libraries\\\\Project\\\\Python\\\\bangkit\\\\landmark_dataset.csv')\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-encoding",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "welcome-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].value_counts().plot.bar()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "official-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-congo",
   "metadata": {},
   "source": [
    "# Remove some class for research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "instructional-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[~df['class'].isin(['f'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-thanks",
   "metadata": {},
   "source": [
    "# Copy to variable\n",
    "Because the variable would refer to the original dataframe. We need to make sure that the original dataframe is intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broke-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.loc[:, df.columns != 'class'].copy()\n",
    "y = df['class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "federal-vatican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>x18</th>\n",
       "      <th>y18</th>\n",
       "      <th>x19</th>\n",
       "      <th>y19</th>\n",
       "      <th>x20</th>\n",
       "      <th>y20</th>\n",
       "      <th>x21</th>\n",
       "      <th>y21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>-0.043678</td>\n",
       "      <td>0.144298</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>0.025318</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>0.045596</td>\n",
       "      <td>0.066051</td>\n",
       "      <td>0.067160</td>\n",
       "      <td>0.047238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017887</td>\n",
       "      <td>-0.144298</td>\n",
       "      <td>-0.067944</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>-0.079499</td>\n",
       "      <td>-0.052633</td>\n",
       "      <td>-0.078473</td>\n",
       "      <td>-0.092986</td>\n",
       "      <td>-0.072233</td>\n",
       "      <td>-0.127822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>0.058144</td>\n",
       "      <td>0.271704</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.235769</td>\n",
       "      <td>-0.022701</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>0.027381</td>\n",
       "      <td>0.102135</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>0.060534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118567</td>\n",
       "      <td>-0.222641</td>\n",
       "      <td>0.115296</td>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.081863</td>\n",
       "      <td>0.060264</td>\n",
       "      <td>0.052515</td>\n",
       "      <td>0.106296</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>0.152769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11129</th>\n",
       "      <td>-0.026221</td>\n",
       "      <td>0.170065</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.151180</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.097461</td>\n",
       "      <td>-0.027577</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>-0.062331</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030867</td>\n",
       "      <td>0.081714</td>\n",
       "      <td>-0.064269</td>\n",
       "      <td>0.054473</td>\n",
       "      <td>-0.076406</td>\n",
       "      <td>0.051963</td>\n",
       "      <td>-0.062771</td>\n",
       "      <td>0.081565</td>\n",
       "      <td>-0.048935</td>\n",
       "      <td>0.097367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>0.109868</td>\n",
       "      <td>0.192643</td>\n",
       "      <td>0.050302</td>\n",
       "      <td>0.195982</td>\n",
       "      <td>-0.003602</td>\n",
       "      <td>0.180525</td>\n",
       "      <td>-0.052067</td>\n",
       "      <td>0.183374</td>\n",
       "      <td>-0.071848</td>\n",
       "      <td>0.165146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011527</td>\n",
       "      <td>-0.195982</td>\n",
       "      <td>0.103197</td>\n",
       "      <td>0.035097</td>\n",
       "      <td>0.112950</td>\n",
       "      <td>-0.037540</td>\n",
       "      <td>0.118199</td>\n",
       "      <td>-0.094426</td>\n",
       "      <td>0.120244</td>\n",
       "      <td>-0.149020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.158974</td>\n",
       "      <td>0.058134</td>\n",
       "      <td>0.118183</td>\n",
       "      <td>0.066401</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>-0.053538</td>\n",
       "      <td>-0.020266</td>\n",
       "      <td>-0.083357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>-0.067907</td>\n",
       "      <td>-0.013207</td>\n",
       "      <td>-0.052015</td>\n",
       "      <td>-0.034669</td>\n",
       "      <td>-0.027278</td>\n",
       "      <td>0.027373</td>\n",
       "      <td>-0.015525</td>\n",
       "      <td>0.065080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>-0.034460</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>0.081525</td>\n",
       "      <td>0.093794</td>\n",
       "      <td>0.035602</td>\n",
       "      <td>0.065331</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045106</td>\n",
       "      <td>-0.101498</td>\n",
       "      <td>-0.085299</td>\n",
       "      <td>-0.017150</td>\n",
       "      <td>-0.093794</td>\n",
       "      <td>-0.088834</td>\n",
       "      <td>-0.085004</td>\n",
       "      <td>-0.103615</td>\n",
       "      <td>-0.072546</td>\n",
       "      <td>-0.083003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>0.036781</td>\n",
       "      <td>0.182181</td>\n",
       "      <td>-0.036370</td>\n",
       "      <td>0.136243</td>\n",
       "      <td>-0.085414</td>\n",
       "      <td>0.064238</td>\n",
       "      <td>-0.111620</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>-0.132566</td>\n",
       "      <td>-0.051478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.069222</td>\n",
       "      <td>0.096775</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>0.117327</td>\n",
       "      <td>-0.077013</td>\n",
       "      <td>0.125644</td>\n",
       "      <td>-0.130845</td>\n",
       "      <td>0.132566</td>\n",
       "      <td>-0.182181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.189145</td>\n",
       "      <td>-0.029989</td>\n",
       "      <td>0.165556</td>\n",
       "      <td>-0.051109</td>\n",
       "      <td>0.101760</td>\n",
       "      <td>-0.021196</td>\n",
       "      <td>0.061260</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.044120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003548</td>\n",
       "      <td>0.073271</td>\n",
       "      <td>0.051109</td>\n",
       "      <td>0.053318</td>\n",
       "      <td>0.034980</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.098282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.156307</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>0.137702</td>\n",
       "      <td>-0.044461</td>\n",
       "      <td>0.101220</td>\n",
       "      <td>-0.032928</td>\n",
       "      <td>0.069205</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.051110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.054616</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.026143</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.045106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>0.099330</td>\n",
       "      <td>0.052320</td>\n",
       "      <td>0.070901</td>\n",
       "      <td>-0.007053</td>\n",
       "      <td>0.033259</td>\n",
       "      <td>-0.035868</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>-0.035316</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>-0.024192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>0.041993</td>\n",
       "      <td>0.041235</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>0.064360</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>0.064802</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.061209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12923 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x1        y1        x2        y2        x3        y3        x4  \\\n",
       "2787  -0.043678  0.144298 -0.001367  0.118090  0.025318  0.085541  0.045596   \n",
       "11587  0.058144  0.271704  0.003272  0.235769 -0.022701  0.156086  0.027381   \n",
       "11129 -0.026221  0.170065  0.004340  0.151180  0.012824  0.097461 -0.027577   \n",
       "2955   0.109868  0.192643  0.050302  0.195982 -0.003602  0.180525 -0.052067   \n",
       "6457   0.003025  0.158974  0.058134  0.118183  0.066401  0.011417  0.021600   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "2303  -0.034460  0.149961  0.020862  0.132600  0.073754  0.081525  0.093794   \n",
       "12535  0.036781  0.182181 -0.036370  0.136243 -0.085414  0.064238 -0.111620   \n",
       "8695   0.006074  0.189145 -0.029989  0.165556 -0.051109  0.101760 -0.021196   \n",
       "1178   0.005577  0.156307 -0.026087  0.137702 -0.044461  0.101220 -0.032928   \n",
       "3724   0.099330  0.052320  0.070901 -0.007053  0.033259 -0.035868 -0.000511   \n",
       "\n",
       "             y4        x5        y5  ...       x17       y17       x18  \\\n",
       "2787   0.066051  0.067160  0.047238  ... -0.017887 -0.144298 -0.067944   \n",
       "11587  0.102135  0.080952  0.060534  ...  0.118567 -0.222641  0.115296   \n",
       "11129  0.059093 -0.062331  0.028613  ... -0.030867  0.081714 -0.064269   \n",
       "2955   0.183374 -0.071848  0.165146  ... -0.011527 -0.195982  0.103197   \n",
       "6457  -0.053538 -0.020266 -0.083357  ...  0.004100  0.016583 -0.067907   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "2303   0.035602  0.065331  0.025165  ... -0.045106 -0.101498 -0.085299   \n",
       "12535 -0.002422 -0.132566 -0.051478  ...  0.036750  0.069222  0.096775   \n",
       "8695   0.061260  0.014569  0.044120  ... -0.003548  0.073271  0.051109   \n",
       "1178   0.069205 -0.010151  0.051110  ...  0.009638  0.054616  0.035243   \n",
       "3724  -0.035316 -0.023747 -0.024192  ... -0.001493  0.041993  0.041235   \n",
       "\n",
       "            y18       x19       y19       x20       y20       x21       y21  \n",
       "2787   0.001256 -0.079499 -0.052633 -0.078473 -0.092986 -0.072233 -0.127822  \n",
       "11587  0.071051  0.081863  0.060264  0.052515  0.106296  0.036721  0.152769  \n",
       "11129  0.054473 -0.076406  0.051963 -0.062771  0.081565 -0.048935  0.097367  \n",
       "2955   0.035097  0.112950 -0.037540  0.118199 -0.094426  0.120244 -0.149020  \n",
       "6457  -0.013207 -0.052015 -0.034669 -0.027278  0.027373 -0.015525  0.065080  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "2303  -0.017150 -0.093794 -0.088834 -0.085004 -0.103615 -0.072546 -0.083003  \n",
       "12535  0.010537  0.117327 -0.077013  0.125644 -0.130845  0.132566 -0.182181  \n",
       "8695   0.053318  0.034980  0.032301  0.016328  0.069721  0.008688  0.098282  \n",
       "1178   0.026143  0.044461 -0.008771  0.040231  0.017964  0.031624  0.045106  \n",
       "3724   0.059598 -0.017817  0.064360 -0.007450  0.064802  0.009451  0.061209  \n",
       "\n",
       "[12923 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spanish-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2787     f\n",
       "11587    w\n",
       "11129    v\n",
       "2955     f\n",
       "6457     m\n",
       "        ..\n",
       "2303     e\n",
       "12535    y\n",
       "8695     r\n",
       "1178     d\n",
       "3724     h\n",
       "Name: class, Length: 12923, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-scroll",
   "metadata": {},
   "source": [
    "# Read class lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "boolean-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe- ter- ber- me- di- se- ke- -an -nya -kan -i a b c d e f g h i j k l m n o p q r s t u v w x y z 0 1 2 3 4 5 6 7 8 9 saya Kamu Dia Selamat pagi sore malam Hai Terima kasih Makan Minum Air Sakit Benar Salah Pergi Tidur Bangun Ajar (buat belajar) Baca Jalan Tulis Lihat Doa Buku Main Beli Baju Hidup Suka Telepon Kenal Ayah Ibu Kakak Adik Saudara Jangan Susu Tinggi Pendek Saat Ingin maaf Asal Untuk Kita Kami dan atau Mereka umur hobi Jam Tanggal Tahun kerja\n"
     ]
    }
   ],
   "source": [
    "classlist = []\n",
    "filename = 'D:\\\\Libraries\\\\Project\\\\Python\\\\bangkit\\\\classlist.txt'\n",
    "with open(filename) as fp:\n",
    "    line = fp.readline()\n",
    "    cnt = 1\n",
    "    while line:\n",
    "        classlist.append(line.strip())\n",
    "        line = fp.readline()\n",
    "        cnt += 1\n",
    "print(*classlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-eleven",
   "metadata": {},
   "source": [
    "# Label encoding and decoding helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulated-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(data_array):\n",
    "    for index, data in data_array.items():\n",
    "        data_array[index] = classlist.index(data)\n",
    "    return np.array(data_array)\n",
    "\n",
    "def decode_label(data_array):\n",
    "    for index, data in enumerate(data_array):\n",
    "        data_array[index] = classlist[data]\n",
    "    return np.array(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collective-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encode_label(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "asian-paraguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-participation",
   "metadata": {},
   "source": [
    "# One hot encoding for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outstanding-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "statistical-draft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-habitat",
   "metadata": {},
   "source": [
    "# Train test split helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "honest-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x,y,train_percent):\n",
    "    x = np.array(x).astype(np.float32)\n",
    "    y = np.array(y).astype(np.float32)\n",
    "    length = len(y)\n",
    "    split = int(length * (train_percent/100))\n",
    "    return x[0:split],y[0:split],x[split:],y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "purple-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = train_test_split(x,y,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-handle",
   "metadata": {},
   "source": [
    "# Get number of classes based on the encoded class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "quiet-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = len(classlist)\n",
    "num_class = len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bearing-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].value_counts().plot.bar()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-welcome",
   "metadata": {},
   "source": [
    "# Build sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "realistic-comedy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b53986ecb00c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_class' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# 42 = 21 landmarks * 2 (x and y)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(42,1)), \n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_class)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-shopping",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "balanced-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-surgeon",
   "metadata": {},
   "source": [
    "# Callback to save best model according to best val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "considered-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"../working/best_run2_epoch{epoch}.h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "subjective-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "batch_size = 256\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-antenna",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "typical-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "41/41 [==============================] - 0s 9ms/step - loss: 3.4082 - acc: 0.2512 - val_loss: 3.1611 - val_acc: 0.3513\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 2.8175 - acc: 0.4364 - val_loss: 2.4594 - val_acc: 0.5319\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 2.1400 - acc: 0.5919 - val_loss: 1.8650 - val_acc: 0.7211\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.6348 - acc: 0.7626 - val_loss: 1.4545 - val_acc: 0.8027\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.2887 - acc: 0.8074 - val_loss: 1.1731 - val_acc: 0.8720\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.0473 - acc: 0.8716 - val_loss: 0.9702 - val_acc: 0.8809\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.8740 - acc: 0.8882 - val_loss: 0.8265 - val_acc: 0.8990\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.7450 - acc: 0.9155 - val_loss: 0.7125 - val_acc: 0.9180\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.6452 - acc: 0.9246 - val_loss: 0.6255 - val_acc: 0.9439\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.5654 - acc: 0.9381 - val_loss: 0.5532 - val_acc: 0.9462\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.5006 - acc: 0.9420 - val_loss: 0.4950 - val_acc: 0.9544\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.4467 - acc: 0.9540 - val_loss: 0.4441 - val_acc: 0.9594\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.4015 - acc: 0.9578 - val_loss: 0.4033 - val_acc: 0.9648\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3629 - acc: 0.9633 - val_loss: 0.3663 - val_acc: 0.9667\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3296 - acc: 0.9653 - val_loss: 0.3361 - val_acc: 0.9687\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3012 - acc: 0.9669 - val_loss: 0.3092 - val_acc: 0.9698\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2763 - acc: 0.9691 - val_loss: 0.2855 - val_acc: 0.9725\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2539 - acc: 0.9710 - val_loss: 0.2636 - val_acc: 0.9721\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2343 - acc: 0.9728 - val_loss: 0.2457 - val_acc: 0.9733\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2169 - acc: 0.9746 - val_loss: 0.2293 - val_acc: 0.9725\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2023 - acc: 0.9754 - val_loss: 0.2139 - val_acc: 0.9752\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1883 - acc: 0.9764 - val_loss: 0.2014 - val_acc: 0.9772\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1766 - acc: 0.9770 - val_loss: 0.1901 - val_acc: 0.9764\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1655 - acc: 0.9783 - val_loss: 0.1789 - val_acc: 0.9776\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1558 - acc: 0.9791 - val_loss: 0.1701 - val_acc: 0.9776\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1469 - acc: 0.9796 - val_loss: 0.1623 - val_acc: 0.9783\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1394 - acc: 0.9806 - val_loss: 0.1551 - val_acc: 0.9803\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1323 - acc: 0.9808 - val_loss: 0.1471 - val_acc: 0.9795\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1259 - acc: 0.9827 - val_loss: 0.1408 - val_acc: 0.9803\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.1197 - acc: 0.9829 - val_loss: 0.1359 - val_acc: 0.9810\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1144 - acc: 0.9832 - val_loss: 0.1308 - val_acc: 0.9834\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1094 - acc: 0.9841 - val_loss: 0.1266 - val_acc: 0.9834\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1049 - acc: 0.9843 - val_loss: 0.1211 - val_acc: 0.9838\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1007 - acc: 0.9848 - val_loss: 0.1169 - val_acc: 0.9841\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0968 - acc: 0.9854 - val_loss: 0.1134 - val_acc: 0.9841\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0932 - acc: 0.9856 - val_loss: 0.1098 - val_acc: 0.9838\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0898 - acc: 0.9858 - val_loss: 0.1069 - val_acc: 0.9841\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0864 - acc: 0.9864 - val_loss: 0.1034 - val_acc: 0.9849\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0836 - acc: 0.9869 - val_loss: 0.1017 - val_acc: 0.9849\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0806 - acc: 0.9867 - val_loss: 0.0982 - val_acc: 0.9849\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0779 - acc: 0.9874 - val_loss: 0.0959 - val_acc: 0.9849\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0754 - acc: 0.9875 - val_loss: 0.0935 - val_acc: 0.9849\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0729 - acc: 0.9875 - val_loss: 0.0918 - val_acc: 0.9849\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0710 - acc: 0.9880 - val_loss: 0.0897 - val_acc: 0.9849\n",
      "Epoch 45/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0686 - acc: 0.9880 - val_loss: 0.0880 - val_acc: 0.9861\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0666 - acc: 0.9886 - val_loss: 0.0855 - val_acc: 0.9857\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0646 - acc: 0.9889 - val_loss: 0.0837 - val_acc: 0.9861\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0631 - acc: 0.9889 - val_loss: 0.0821 - val_acc: 0.9861\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0610 - acc: 0.9891 - val_loss: 0.0814 - val_acc: 0.9861\n",
      "Epoch 50/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0596 - acc: 0.9889 - val_loss: 0.0788 - val_acc: 0.9861\n",
      "Epoch 51/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0581 - acc: 0.9895 - val_loss: 0.0785 - val_acc: 0.9865\n",
      "Epoch 52/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0561 - acc: 0.9896 - val_loss: 0.0765 - val_acc: 0.9865\n",
      "Epoch 53/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0545 - acc: 0.9899 - val_loss: 0.0749 - val_acc: 0.9868\n",
      "Epoch 54/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0532 - acc: 0.9898 - val_loss: 0.0746 - val_acc: 0.9868\n",
      "Epoch 55/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0520 - acc: 0.9899 - val_loss: 0.0733 - val_acc: 0.9868\n",
      "Epoch 56/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0506 - acc: 0.9902 - val_loss: 0.0723 - val_acc: 0.9868\n",
      "Epoch 57/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0495 - acc: 0.9900 - val_loss: 0.0710 - val_acc: 0.9868\n",
      "Epoch 58/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0480 - acc: 0.9909 - val_loss: 0.0702 - val_acc: 0.9876\n",
      "Epoch 59/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0468 - acc: 0.9910 - val_loss: 0.0689 - val_acc: 0.9872\n",
      "Epoch 60/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0458 - acc: 0.9907 - val_loss: 0.0675 - val_acc: 0.9872\n",
      "Epoch 61/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0444 - acc: 0.9917 - val_loss: 0.0669 - val_acc: 0.9876\n",
      "Epoch 62/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0434 - acc: 0.9920 - val_loss: 0.0656 - val_acc: 0.9876\n",
      "Epoch 63/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0423 - acc: 0.9914 - val_loss: 0.0648 - val_acc: 0.9876\n",
      "Epoch 64/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0416 - acc: 0.9919 - val_loss: 0.0646 - val_acc: 0.9876\n",
      "Epoch 65/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0404 - acc: 0.9921 - val_loss: 0.0634 - val_acc: 0.9876\n",
      "Epoch 66/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0394 - acc: 0.9926 - val_loss: 0.0624 - val_acc: 0.9876\n",
      "Epoch 67/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0386 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9880\n",
      "Epoch 68/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0377 - acc: 0.9930 - val_loss: 0.0620 - val_acc: 0.9876\n",
      "Epoch 69/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0369 - acc: 0.9927 - val_loss: 0.0605 - val_acc: 0.9876\n",
      "Epoch 70/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0360 - acc: 0.9934 - val_loss: 0.0599 - val_acc: 0.9880\n",
      "Epoch 71/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0353 - acc: 0.9932 - val_loss: 0.0597 - val_acc: 0.9884\n",
      "Epoch 72/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0347 - acc: 0.9937 - val_loss: 0.0583 - val_acc: 0.9880\n",
      "Epoch 73/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0338 - acc: 0.9939 - val_loss: 0.0583 - val_acc: 0.9888\n",
      "Epoch 74/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0332 - acc: 0.9942 - val_loss: 0.0573 - val_acc: 0.9884\n",
      "Epoch 75/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0325 - acc: 0.9943 - val_loss: 0.0566 - val_acc: 0.9884\n",
      "Epoch 76/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0317 - acc: 0.9942 - val_loss: 0.0562 - val_acc: 0.9888\n",
      "Epoch 77/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0311 - acc: 0.9942 - val_loss: 0.0554 - val_acc: 0.9896\n",
      "Epoch 78/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0306 - acc: 0.9946 - val_loss: 0.0548 - val_acc: 0.9888\n",
      "Epoch 79/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0300 - acc: 0.9948 - val_loss: 0.0547 - val_acc: 0.9896\n",
      "Epoch 80/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0294 - acc: 0.9948 - val_loss: 0.0548 - val_acc: 0.9896\n",
      "Epoch 81/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0289 - acc: 0.9949 - val_loss: 0.0534 - val_acc: 0.9903\n",
      "Epoch 82/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0282 - acc: 0.9952 - val_loss: 0.0527 - val_acc: 0.9896\n",
      "Epoch 83/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0276 - acc: 0.9952 - val_loss: 0.0523 - val_acc: 0.9888\n",
      "Epoch 84/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0276 - acc: 0.9952 - val_loss: 0.0522 - val_acc: 0.9899\n",
      "Epoch 85/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0268 - acc: 0.9956 - val_loss: 0.0515 - val_acc: 0.9903\n",
      "Epoch 86/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0262 - acc: 0.9956 - val_loss: 0.0511 - val_acc: 0.9907\n",
      "Epoch 87/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0257 - acc: 0.9955 - val_loss: 0.0508 - val_acc: 0.9903\n",
      "Epoch 88/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0253 - acc: 0.9956 - val_loss: 0.0504 - val_acc: 0.9907\n",
      "Epoch 89/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0248 - acc: 0.9958 - val_loss: 0.0496 - val_acc: 0.9903\n",
      "Epoch 90/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0245 - acc: 0.9955 - val_loss: 0.0495 - val_acc: 0.9903\n",
      "Epoch 91/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0241 - acc: 0.9954 - val_loss: 0.0491 - val_acc: 0.9911\n",
      "Epoch 92/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0236 - acc: 0.9961 - val_loss: 0.0487 - val_acc: 0.9911\n",
      "Epoch 93/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0231 - acc: 0.9961 - val_loss: 0.0484 - val_acc: 0.9911\n",
      "Epoch 94/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0227 - acc: 0.9962 - val_loss: 0.0481 - val_acc: 0.9911\n",
      "Epoch 95/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0223 - acc: 0.9964 - val_loss: 0.0480 - val_acc: 0.9911\n",
      "Epoch 96/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0220 - acc: 0.9964 - val_loss: 0.0475 - val_acc: 0.9911\n",
      "Epoch 97/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0217 - acc: 0.9962 - val_loss: 0.0468 - val_acc: 0.9911\n",
      "Epoch 98/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0214 - acc: 0.9964 - val_loss: 0.0470 - val_acc: 0.9911\n",
      "Epoch 99/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0209 - acc: 0.9962 - val_loss: 0.0463 - val_acc: 0.9911\n",
      "Epoch 100/100\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0207 - acc: 0.9968 - val_loss: 0.0460 - val_acc: 0.9919\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train,y=y_train,epochs=num_epoch,batch_size=batch_size,validation_data=(x_test,y_test), verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-organ",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "skilled-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-terry",
   "metadata": {},
   "source": [
    "# Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "external-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-2770307e385a>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict_classes(np.array(x_train[:1]).astype(np.float32))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "failing-princeton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.argmax(tf.keras.utils.to_categorical(prediction), axis = 1)[0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "recent-commodity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6.0565501e-02,  1.6469809e-01, -2.7848274e-02,  1.1718124e-01,\n",
       "        -8.1218094e-02,  2.3503363e-02, -9.2865318e-02, -8.3658844e-02,\n",
       "        -9.3911082e-02, -1.6469809e-01, -5.0614029e-02, -6.1340868e-02,\n",
       "        -5.3317934e-02, -1.4779785e-01, -3.8688332e-02, -6.1840087e-02,\n",
       "        -3.8520008e-02, -3.9240003e-02, -3.1355917e-03, -6.7572534e-02,\n",
       "        -2.7942359e-03, -1.4334139e-01,  6.9002211e-03, -3.6277145e-02,\n",
       "        -8.7946653e-05, -4.0533423e-02,  4.4508725e-02, -6.6564098e-02,\n",
       "         4.4652075e-02, -1.4053971e-01,  4.5145184e-02, -4.0652275e-02,\n",
       "         4.1062385e-02, -3.7139118e-02,  9.3911082e-02, -6.3033313e-02,\n",
       "         8.8246435e-02, -1.2895179e-01,  8.1502885e-02, -5.9864491e-02,\n",
       "         7.9411894e-02, -4.4073403e-02], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "general-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n"
     ]
    }
   ],
   "source": [
    "print(decode_label([prediction]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
