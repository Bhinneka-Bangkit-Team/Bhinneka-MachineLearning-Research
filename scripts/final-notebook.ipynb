{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Script.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z5UZurx8h2Fl",
        "RmErxw76izK-",
        "lpnAgZsGkkyO",
        "liat1l60lV73",
        "UC57ZWwPyj6n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UZurx8h2Fl"
      },
      "source": [
        "### Necessary Packages\n",
        "\n",
        "*   Pandas\n",
        "*   Numpy\n",
        "*   Mediapipe\n",
        "*   OpenCV\n",
        "*   OS\n",
        "*   Shutil\n",
        "*   Random\n",
        "*   TensorFlow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsZ_hNrvhwDA"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcbUxd0jiAVX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras import Model \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmErxw76izK-"
      },
      "source": [
        "### Image Pre-Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzPa5HZZiimq"
      },
      "source": [
        "def crop_img (path):\n",
        "  '''function for detecting and cropping hands from an image.\n",
        "     if there is no hand detected, it will delete that image.'''\n",
        "\n",
        "  mp_hands = mp.solutions.hands\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "  with mp_hands.Hands(static_image_mode=True) as hands:\n",
        "\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (540, 960))\n",
        "    height, width, color = img.shape\n",
        "    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if results.multi_hand_landmarks != None:\n",
        "\n",
        "      x_max = 0\n",
        "      y_max = 0\n",
        "      x_min = width\n",
        "      y_min = height\n",
        "      tol = 25\n",
        "\n",
        "      for handLandmarks in results.multi_hand_landmarks:\n",
        "        for landmarks in handLandmarks.landmark:\n",
        "          x, y = int(landmarks.x * width), int(landmarks.y * height)\n",
        "          if x > x_max:\n",
        "              x_max = x\n",
        "          if x < x_min:\n",
        "              x_min = x\n",
        "          if y > y_max:\n",
        "              y_max = y\n",
        "          if y < y_min:\n",
        "              y_min = y\n",
        "\n",
        "      coor = [max(x_min - tol, 0), min(x_max+ tol, 540), \n",
        "              max(y_min - tol, 0), min(y_max + tol, 960)]\n",
        "      \n",
        "      x_min, x_max, y_min, y_max = [round(num) for num in coor]\n",
        "\n",
        "      cropped_img = img[y_min : y_max, x_min : x_max]\n",
        "      cv2.imwrite(path, cropped_img)\n",
        "    \n",
        "    else:\n",
        "      os.remove(path)\n",
        "      print(path, 'No landmark found. File Deleted.')\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlGB02c1jpHu"
      },
      "source": [
        "path = './Dataset'\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "  src = os.path.join(path, folder)\n",
        "  for img in os.listdir(src):\n",
        "    name = os.path.join(src, img)\n",
        "    crop_img(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcpmh3N8j2o-"
      },
      "source": [
        "def pad (img_path):\n",
        "  ''' This function will pad and resize the image to (150, 150, 3) shape.'''\n",
        "\n",
        "  img = cv2.imread(img_path)\n",
        "  width, length, channel = img.shape\n",
        "  size = max([width, length])\n",
        "\n",
        "  top = round((size - width)/2)\n",
        "  left = round((size - length)/2)\n",
        "  bottom = top\n",
        "  right = left\n",
        "\n",
        "  img = cv2.copyMakeBorder(img, top = top, bottom = bottom,\n",
        "                           left = left, right = right,\n",
        "                           borderType = cv2.BORDER_CONSTANT,\n",
        "                           value = [0, 0, 0])\n",
        "  \n",
        "  img = cv2.resize(img, (150, 150))\n",
        "\n",
        "  cv2.imwrite(img_path, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvY1dkImj3WA"
      },
      "source": [
        "path = './Dataset'\n",
        "\n",
        "for folders in os.listdir(path):\n",
        "  for img in os.listdir(os.path.join(path, folders)):\n",
        "    pad(os.path.join(path, folders, img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpnAgZsGkkyO"
      },
      "source": [
        "### Splitting Data into Training and Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oubL1sVckoAP"
      },
      "source": [
        "os.mkdir('./SplitData')\n",
        "os.mkdir('./SplitData/train')\n",
        "os.mkdir('./SplitData/val')\n",
        "\n",
        "for classes in os.listdir('./Dataset'):\n",
        "  os.mkdir(os.path.join('./Split/train', classes))\n",
        "  os.mkdir(os.path.join('./Split/val', classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNfQkPQKkoeV"
      },
      "source": [
        "def data_split (train_perc, source, dest_train, dest_val):\n",
        "  '''Shuffle and split data into training and validation set'''\n",
        "\n",
        "  for folders in os.listdir(source):\n",
        "    fol_path = os.path.join(source, folders)\n",
        "    files = os.listdir(fol_path)\n",
        "    random.shuffle(files)\n",
        "\n",
        "    num_of_files = len(files)\n",
        "    train_idx = round(train_perc * num_of_files)\n",
        "\n",
        "    for file_ in files[0:train_idx]:\n",
        "      origin = os.path.join(fol_path, file_)\n",
        "      dest = os.path.join(dest_train, folders, file_)\n",
        "      shutil.copy(origin, dest)\n",
        "    \n",
        "    for file_ in files[train_idx:]:\n",
        "      origin = os.path.join(fol_path, file_)\n",
        "      dest = os.path.join(dest_val, folders, file_)\n",
        "\n",
        "      shutil.copy(origin, dest)\n",
        "\n",
        "data_split(0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ikKulQk-iV"
      },
      "source": [
        "source = './Dataset'\n",
        "dest_train = './Split/train'\n",
        "dest_val = './Split/val'\n",
        "data_split(0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xwOfBPglZTl"
      },
      "source": [
        "#Checking the number of training and validation data\n",
        "\n",
        "train_img = 0\n",
        "val_img = 0\n",
        "\n",
        "for classes in os.listdir(dest_train):\n",
        "    train_img += len(os.listdir(os.path.join(dest_train, classes)))\n",
        "    val_img += len(os.listdir(os.path.join(dest_val, classes)))\n",
        "\n",
        "print(train_img)\n",
        "print(val_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liat1l60lV73"
      },
      "source": [
        "### Creating and Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_j-3z3lr9n"
      },
      "source": [
        "# Getting Inception V3 pre-trained model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O ./tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1tiLILlucg"
      },
      "source": [
        "# Using ImageDataGenerator to generate the training and validation set\n",
        "\n",
        "base_dir = './Split'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 20,\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    target_size = (150, 150))\n",
        "\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size  = 10,\n",
        "                                                         class_mode  = 'categorical',\n",
        "                                                         target_size = (150,150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxZZrRpKl3nB"
      },
      "source": [
        "# Load the Inception V3 Model\n",
        "\n",
        "local_weight = './tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weight)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmgM_TUHmPM5"
      },
      "source": [
        "# Creating the Final Model\n",
        "\n",
        "last_out = pre_trained_model.get_layer('mixed10').output\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(last_out)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(1024, activation = 'relu')(x)\n",
        "x = layers.Dense(num_classes, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "model.compile(optimizer = Adam(lr = 0.0001),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF4aQFSrmGY3"
      },
      "source": [
        "# Creating Callbacks that saves epoch with the highest Validation Accuracy\n",
        "\n",
        "checkpoint_filepath = './Checkpoint/model_cp.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uok8FVhYl68T"
      },
      "source": [
        "#Train the Model\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 100,\n",
        "                    verbose = 2,\n",
        "                    callbacks = [model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RInM60srmswf"
      },
      "source": [
        "# Load the saved weight (from the callbacks), evaluate the scores using\n",
        "# validation set, and save the model\n",
        "\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "scores = model.evaluate(validation_generator)\n",
        "print(f'val_loss: {scores[0]}, val_accuracy: {scores[1]}')\n",
        "\n",
        "model.save('./final_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NZO7hmpmeKb"
      },
      "source": [
        "### Results and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Dr6cJ1mc_a"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GA_1xSAmpZB"
      },
      "source": [
        "# Create a label dictionary to convert model prediction result to the word\n",
        "\n",
        "word_list = []\n",
        "for word in (os.listdir('./Split/train')):\n",
        "    word_list.append(word)\n",
        "\n",
        "word_list.sort()\n",
        "\n",
        "for index, word in enumerate(word_list):\n",
        "  label_dict[index] = word\n",
        "print(label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rigH9BbMoKDE"
      },
      "source": [
        "def pred(path):\n",
        "  '''Function for processing and predicting images'''\n",
        "  img = image.load_img(path, target_size=(150, 150, 3))\n",
        "  img = image.img_to_array(img)\n",
        "  img = img/255.0\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  return label_dict[np.argmax(model.predict(img), axis = 1)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kihLHrkQoLkD"
      },
      "source": [
        "#Creating a Confusion Matrix DataFrame, The columns are actual word, and the rows are predicted words\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "df = pd.DataFrame(columns = label_dict.values(),\n",
        "                  index = label_dict.values())\n",
        "\n",
        "df.replace(np.NaN, 0, inplace = True)\n",
        "\n",
        "path = './Split/val'\n",
        "for folders in os.listdir(path):\n",
        "  for imgs in os.listdir(os.path.join(path, folders)):\n",
        "    y = pred(os.path.join(path, folders, imgs))\n",
        "    df.loc[y, folders] += 1\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfdwNEUNo2cb"
      },
      "source": [
        "#### Function for processing an actual Image to match with the model input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-1m5DUIo-5s"
      },
      "source": [
        "def img_convert (path):\n",
        "  '''Processing images for prediction'''\n",
        "\n",
        "  mp_hands = mp.solutions.hands\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "  \n",
        "  with mp_hands.Hands(static_image_mode=True) as hands:\n",
        "\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (540, 960))\n",
        "\n",
        "    height, width, color = img.shape\n",
        "    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if results.multi_hand_landmarks != None:\n",
        "\n",
        "        x_max = 0\n",
        "        y_max = 0\n",
        "        x_min = width\n",
        "        y_min = height\n",
        "        tol = 25\n",
        "\n",
        "        for handLandmarks in results.multi_hand_landmarks:\n",
        "          for landmarks in handLandmarks.landmark:\n",
        "            x, y = int(landmarks.x * width), int(landmarks.y * height)\n",
        "            if x > x_max:\n",
        "                x_max = x\n",
        "            if x < x_min:\n",
        "                x_min = x\n",
        "            if y > y_max:\n",
        "                y_max = y\n",
        "            if y < y_min:\n",
        "                y_min = y \n",
        "        \n",
        "        coor = [max(x_min - tol, 0), min(x_max+ tol, 540), \n",
        "                max(y_min - tol, 0), min(y_max + tol, 960)]\n",
        "\n",
        "        x_min, x_max, y_min, y_max = [round(num) for num in coor]\n",
        "\n",
        "        cropped_img = img[y_min : y_max, x_min : x_max]\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "  width, length, channel = cropped_img.shape\n",
        "\n",
        "  size = max([width, length])\n",
        "  top = round((size - width)/2)\n",
        "  left = round((size - length)/2)\n",
        "  bottom = top\n",
        "  right = left\n",
        "\n",
        "  cropped_img = cv2.copyMakeBorder(cropped_img, top = top, bottom = bottom,\n",
        "                                   left = left, right = right,\n",
        "                                   borderType = cv2.BORDER_CONSTANT,\n",
        "                                   value = [0, 0, 0])\n",
        "  \n",
        "  cropped_img = cv2.resize(cropped_img, (300, 300))\n",
        "  cropped_img = cropped_img / 255\n",
        "  cropped_img = np.expand_dims(cropped_img, axis=0)\n",
        "  \n",
        "  return blackFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewIohctyY7w_"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpRP3_hNZCiT"
      },
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        " \n",
        "capture = cv2.VideoCapture(0)\n",
        "\n",
        "prediction_pool = np.zeros((25,))\n",
        "frame_count = 1\n",
        "\n",
        "with mp_hands.Hands(static_image_mode=False, \n",
        "                    min_detection_confidence=0.7, \n",
        "                    min_tracking_confidence=0.7, max_num_hands=2) as hands:\n",
        "    \n",
        "    while (True):\n",
        "        ret, frame = capture.read()\n",
        "        frame = cv2.flip(frame,1)\n",
        "        \n",
        "        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        height, width, color = frame.shape\n",
        "        \n",
        "        if results.multi_hand_landmarks != None:\n",
        "            x_max = 0\n",
        "            y_max = 0\n",
        "            x_min = width\n",
        "            y_min = height\n",
        "            tol = 25\n",
        "            \n",
        "            for handLandmarks in results.multi_hand_landmarks:\n",
        "                \n",
        "                for landmarks in handLandmarks.landmark:\n",
        "                    x, y = int(landmarks.x * width), int(landmarks.y * height)\n",
        "                    if x > x_max:\n",
        "                        x_max = x\n",
        "                    if x < x_min:\n",
        "                        x_min = x\n",
        "                    if y > y_max:\n",
        "                        y_max = y\n",
        "                    if y < y_min:\n",
        "                        y_min = y\n",
        "            \n",
        "            coor = [max(x_min - tol, 0), min(x_max + tol, width), \n",
        "                    max(y_min - tol, 0), min(y_max + tol, height)]\n",
        "            \n",
        "            x_min, x_max, y_min, y_max = [round(num) for num in coor]\n",
        "            \n",
        "            inp_frame = frame[y_min : y_max, x_min : x_max]\n",
        "            inp_frame = pad_frame(inp_frame)\n",
        "            max_pred = np.max(model.predict(inp_frame)).squeeze()\n",
        "            \n",
        "            if max_pred >= 0.75:\n",
        "                pred_idx = np.argmax(model.predict(inp_frame), axis = 1)[0]\n",
        "                prediction_pool[pred_idx] += 1\n",
        "                frame_count += 1\n",
        "            \n",
        "                cv2.putText(frame, label_dict[pred_idx], (round((x_min + x_max)/2), y_min - 10), \n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            \n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
        "\n",
        "        cv2.imshow('Test hand', frame)\n",
        "        if frame_count % 45 == 0:\n",
        "            result = np.argmax(prediction_pool).squeeze()\n",
        "            print(label_dict[result], end=' ')\n",
        "            prediction_pool = np.zeros((25,))\n",
        "                \n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        " \n",
        "    cv2.destroyAllWindows()\n",
        "    capture.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC57ZWwPyj6n"
      },
      "source": [
        "### Converting to TFLite Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb0OyiRryjXI"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"./Final_model_default.tflite\", \"wb\") as file:\n",
        "    file.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}